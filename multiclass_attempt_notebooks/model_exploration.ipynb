{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data and prep it for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea in this notebook is to get a sense for how various models perform.  The assessment is based simply on the train and test accuracy.\n",
    "\n",
    "In another notebook the models will be evaluated with cross validation and compared with grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# machine learning stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# my utilities\n",
    "from crash_utils.zip_code_and_borough_from_coords import zip_code_and_borough_from_coords\n",
    "from crash_utils.fix_vehicle_names import fix_vehicle_names\n",
    "from crash_utils.make_crash_features import make_crash_features\n",
    "from crash_utils.basic_cleaning import basic_cleaning\n",
    "from crash_utils.prepare_data_for_modelling import prepare_data_for_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/Mark/brainstation/capstone/nyc_bike_crash_analysis/data/\"\n",
    "df = pd.read_csv(data_path + \"Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing zip coded and boroughs using lat/lon\n",
    "df = zip_code_and_borough_from_coords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some basic data munging operations (see `crash_utils/basic_cleaning.py` for details)\n",
    "df = basic_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the VEHICLE TYPE CODE columns\n",
    "df = fix_vehicle_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modelling\n",
    "# drop columns\n",
    "# set up target\n",
    "# run \"make_crash_features.py\"\n",
    "# OHE the text columns\n",
    "# count-vectorize the vehicles and crash factors\n",
    "\n",
    "df = prepare_data_for_modelling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test some models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the features and targets from the big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "# also scale the data for those algorithms which would benefit (e.g., KNN)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train, y_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit DTC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth = 20).fit(X_train, y_train)\n",
    "\n",
    "dtc.fit(X_train,y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(dtc.score(X_train, y_train),3))\n",
    "print(round(dtc.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = dtc.predict(X_test)\n",
    "y_pred_train= dtc.predict(X_train)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1','True Class 2'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1','Predicted Class 2'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(y_test.value_counts().sort_index())\n",
    "counts_df.rename(columns = {\"outcome\":\"number\"}, inplace=True)\n",
    "counts_df[\"outcome\"] = [\"no injury\",\"injury\",\"fatality\"]\n",
    "counts_df.index.name = \"encoding\"\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(conf_matrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__little mini hyperparameter search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depths = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "depths = np.arange(10,200,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for depth in depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth = depth)\n",
    "    dtc.fit(X_train,y_train)\n",
    "    test_score.append(dtc.score(X_test, y_test))\n",
    "    train_score.append(dtc.score(X_train, y_train))\n",
    "    print(depth,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(depths, train_score, label='training set', marker='o')\n",
    "plt.plot(depths, test_score, label='test set', marker='o')\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"DTC: impact of max depth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_leafs = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "min_leafs = np.arange(10,200,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for leaf in min_leafs:\n",
    "    dtc = DecisionTreeClassifier(max_depth = 180, min_samples_leaf = leaf)\n",
    "    dtc.fit(X_train,y_train)\n",
    "    test_score.append(dtc.score(X_test, y_test))\n",
    "    train_score.append(dtc.score(X_train, y_train))\n",
    "    print(leaf,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(min_leafs, train_score, label='training set', marker='o')\n",
    "plt.plot(min_leafs, test_score, label='test set', marker='o')\n",
    "#plt.xscale(\"log\")\n",
    "plt.xlabel('min samples per leaf')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"DTC: impact of min. samples per leaf\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# optimized RF classifier\n",
    "rf = RandomForestClassifier(max_depth = 50, n_estimators = 100, n_jobs = -1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(rf.score(X_train, y_train),3))\n",
    "print(round(rf.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(10,100,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    rf = RandomForestClassifier(n_estimators = estimator, max_depth = 150, n_jobs=-1)\n",
    "    rf.fit(X_train,y_train)\n",
    "    test_score.append(rf.score(X_test, y_test))\n",
    "    train_score.append(rf.score(X_train, y_train))\n",
    "    print(estimator,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(estimators, train_score, label='training set', marker='o')\n",
    "plt.plot(estimators, test_score, label='test set', marker='o')\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Random forest: impact of number of estimators\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(gnb.score(X_train_scaled, y_train),3))\n",
    "print(round(gnb.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = gnb.predict(X_test_scaled)\n",
    "y_pred_train= gnb.predict(X_train_scaled)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1','True Class 2'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1','Predicted Class 2'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(gb.score(X_train, y_train),3))\n",
    "print(round(gb.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(10,200,20)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    gb = GradientBoostingClassifier(n_estimators = estimator)\n",
    "    gb.fit(X_train,y_train)\n",
    "    test_score.append(gb.score(X_test, y_test))\n",
    "    train_score.append(gb.score(X_train, y_train))\n",
    "    print(estimator,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(estimators, train_score, label='training set', marker='o')\n",
    "plt.plot(estimators, test_score, label='test set', marker='o')\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Random forest: impact of number of estimators\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#fitting Adaptive Boosting\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X, y)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(ada.score(X_train, y_train),3))\n",
    "print(round(ada.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(knn.score(X_train_scaled, y_train),3))\n",
    "print(round(knn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(20,100,500,100,20), solver='lbfgs')\n",
    "\n",
    "nn.fit(X_train_scaled,y_train)\n",
    "\n",
    "print(round(nn.score(X_train_scaled, y_train),3))\n",
    "print(round(nn.score(X_test_scaled, y_test),3))\n",
    "\n",
    "\n",
    "#nn.fit(X_train_scaled,y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "# print(round(nn.score(X_train_scaled, y_train),3))\n",
    "# print(round(nn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = nn.predict(X_test_scaled)\n",
    "y_pred_train= nn.predict(X_train_scaled)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1','True Class 2'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1','Predicted Class 2'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "#model.add(layers.Dense(500, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train, y_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "type(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "\n",
    "result = model.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {result[1]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
