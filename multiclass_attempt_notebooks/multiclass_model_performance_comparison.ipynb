{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data and prep it for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea in this notebook is to get a sense for how various models perform.  The assessment is based simply on the train and test accuracy.\n",
    "\n",
    "In another notebook the models will be evaluated with cross validation and compared with grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# some other useful things\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import datetime\n",
    "from os import path\n",
    "\n",
    "# machine learning stuff\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# my utilities\n",
    "from crash_utils.zip_code_and_borough_from_coords import zip_code_and_borough_from_coords\n",
    "from crash_utils.fix_vehicle_names import fix_vehicle_names\n",
    "from crash_utils.make_crash_features import make_crash_features\n",
    "from crash_utils.basic_cleaning import basic_cleaning\n",
    "from crash_utils.prepare_data_for_modelling import prepare_data_for_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/Mark/brainstation/capstone/nyc_bike_crash_analysis/data/\"\n",
    "df = pd.read_csv(data_path + \"Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing zip coded and boroughs using lat/lon\n",
    "df = zip_code_and_borough_from_coords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some basic data munging operations (see `crash_utils/basic_cleaning.py` for details)\n",
    "df = basic_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the VEHICLE TYPE CODE columns\n",
    "df = fix_vehicle_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modelling\n",
    "# drop columns\n",
    "# set up target\n",
    "# run \"make_crash_features.py\"\n",
    "# OHE the text columns\n",
    "# count-vectorize the vehicles and crash factors\n",
    "\n",
    "df = prepare_data_for_modelling(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract the features and targets from the big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "# also scale the data for those algorithms which would benefit (e.g., KNN)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train, y_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now build the pipeline for grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2020-12-06, \n",
    "# accuracy score of \n",
    "# file: \n",
    "\n",
    "file_name = \"2\"\n",
    "\n",
    "if path.exists(file_name):\n",
    "\n",
    "    infile = open(file_name,\"rb\")\n",
    "    grid_out = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "else:\n",
    "\n",
    "    # pipeline initiation\n",
    "    steps = [('dim_reduction', PCA()),            # step 2: PCA\n",
    "             ('model', RandomForestClassifier())] # step 3: fit a regressor model\n",
    "        \n",
    "    model_pipeline = Pipeline(steps)\n",
    "    \n",
    "    # model parameters\n",
    "    forest_params = {'model': [RandomForestClassifier(max_depth=50, n_estimators=100)],\n",
    "                     'dim_reduction': [PCA()],\n",
    "                     'dim_reduction__n_components':[70]\n",
    "                    }\n",
    "\n",
    "\n",
    "    # create our grid\n",
    "    parameter_grid = [forest_params]\n",
    "    grid_out = GridSearchCV(model_pipeline, parameter_grid, cv=5, verbose=1, n_jobs = -1, \n",
    "                            scoring = \"f1_micro\")\n",
    "    \n",
    "    # fit\n",
    "    grid_out.fit(X_remainder, y_remainder)\n",
    "     \n",
    "    # write results    \n",
    "    file_name = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" GridCVresults.pkl\"\n",
    "    pickle.dump(grid_out, open(file_name, \"wb\") )\n",
    "    print(\"wrote:\",file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_out.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = grid_out.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view performance of all models\n",
    "grid_out.cv_results_.keys()\n",
    "#grid_out.cv_results_[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(grid_out.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other models to add:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(gnb.score(X_train_scaled, y_train),3))\n",
    "print(round(gnb.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#fitting Adaptive Boosting\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(ada.score(X_train, y_train),3))\n",
    "print(round(ada.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "rc = RidgeClassifier()\n",
    "rc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(rc.score(X_train_scaled, y_train),3))\n",
    "print(round(rc.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(knn.score(X_train_scaled, y_train),3))\n",
    "print(round(knn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(20,100,500,100,20), solver='lbfgs')\n",
    "\n",
    "nn.fit(X_train_scaled,y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(nn.score(X_train_scaled, y_train),3))\n",
    "print(round(nn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "#model.add(layers.Dense(500, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    \n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train, y_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "type(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "\n",
    "result = model.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {result[1]:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2020-12-06, best is RandomForestClassifier(max_depth = 50, n_estimators = 100)\n",
    "# accuracy score of 82.6%\n",
    "# file: 2020-12-06 20:46:08 GridCVresults.pkl\n",
    "\n",
    "file_name = \"2020-12-06 20:46:08 GridCVresults.pkl\"\n",
    "\n",
    "if path.exists(file_name):\n",
    "\n",
    "    infile = open(file_name,\"rb\")\n",
    "    grid_out = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    else:\n",
    "        \n",
    "    # pipeline initiation\n",
    "    steps = [('model', DecisionTreeClassifier())]\n",
    "    model_pipeline = Pipeline(steps)\n",
    "\n",
    "\n",
    "\n",
    "    tree_params = {'model': [DecisionTreeClassifier()],\n",
    "                   'model__max_depth': np.arange(10,200,10),\n",
    "                   'model__min_samples_leaf': np.arange(10,200,10)}\n",
    "\n",
    "\n",
    "    forest_params = {'model': [RandomForestClassifier()],\n",
    "                     'model__max_depth': np.arange(10,200,10),\n",
    "                     'model__n_estimators': [20, 50, 100],\n",
    "                    }\n",
    "\n",
    "    gboost_params = {'model': [GradientBoostingClassifier()],\n",
    "                     'model__learning_rate': [0.1, 0.5, 0.9],\n",
    "                     'model__n_estimators': [20, 50, 100, 200]}\n",
    "\n",
    "    # create our grid\n",
    "\n",
    "    parameter_grid = [tree_params, forest_params, gboost_params]\n",
    "    grid_out = GridSearchCV(model_pipeline, parameter_grid, cv=5, verbose=1, n_jobs = -1, scoring=\"accuracy\")\n",
    "    \n",
    "    # fit\n",
    "    grid_out.fit(X_remainder, y_remainder)\n",
    "    \n",
    "    file_name = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" GridCVresults.pkl\"\n",
    "    pickle.dump(grid_out, open(file_name, \"wb\") )\n",
    "    print(\"wrote:\",file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2020-12-06, best is PCA(n_components=50), RandomForestClassifier(max_depth=50), no scaler\n",
    "# accuracy score of 91.7%\n",
    "# file: 2020-12-06 22:32:26 GridCVresults.pkl\n",
    "\n",
    "file_name = \"2020-12-06 22:32:26 GridCVresults.pkl\"\n",
    "\n",
    "if path.exists(file_name):\n",
    "\n",
    "    infile = open(file_name,\"rb\")\n",
    "    grid_out = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "else:\n",
    "\n",
    "    # pipeline initiation\n",
    "\n",
    "    steps = [('scaler', StandardScaler()),  # step 1: scale\n",
    "             ('dim_reduction', PCA()),      # step 2: PCA\n",
    "             ('model', RandomForestClassifier())] # step 3: fit a regressor model\n",
    "        \n",
    "    model_pipeline = Pipeline(steps)\n",
    "    \n",
    "    # model parameters\n",
    "    forest_params = {'model': [RandomForestClassifier(max_depth=50, n_estimators=100)],\n",
    "                     'scaler': [None, MinMaxScaler(), StandardScaler()],\n",
    "                     'dim_reduction': [PCA()],\n",
    "                     'dim_reduction__n_components':[1, 2, 5, 10, 20, 50, 100, 200],\n",
    "                     \n",
    "                    }\n",
    "\n",
    "\n",
    "    # create our grid\n",
    "    parameter_grid = [forest_params]\n",
    "    grid_out = GridSearchCV(model_pipeline, parameter_grid, cv=5, verbose=1, n_jobs = -1, scoring=\"accuracy\")\n",
    "    \n",
    "    # fit\n",
    "    grid_out.fit(X_remainder, y_remainder)\n",
    "     \n",
    "    # write results    \n",
    "    file_name = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" GridCVresults.pkl\"\n",
    "    pickle.dump(grid_out, open(file_name, \"wb\") )\n",
    "    print(\"wrote:\",file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2020-12-06, best is PCA(n_components=70), RandomForestClassifier(max_depth=50)}\n",
    "# accuracy score of 92.7\n",
    "# file: 2020-12-06 22:57:34 GridCVresults.pkl\n",
    "\n",
    "file_name = \"2020-12-06 22:57:34 GridCVresults.pkl\"\n",
    "\n",
    "if path.exists(file_name):\n",
    "\n",
    "    infile = open(file_name,\"rb\")\n",
    "    grid_out = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "else:\n",
    "\n",
    "    # pipeline initiation\n",
    "    steps = [('dim_reduction', PCA()),      # step 2: PCA\n",
    "             ('model', RandomForestClassifier())] # step 3: fit a regressor model\n",
    "        \n",
    "    model_pipeline = Pipeline(steps)\n",
    "    \n",
    "    # model parameters\n",
    "    forest_params = {'model': [RandomForestClassifier(max_depth=50, n_estimators=100)],\n",
    "                     'dim_reduction': [PCA()],\n",
    "                     'dim_reduction__n_components':[20, 30, 50, 60, 70, 80]\n",
    "                    }\n",
    "\n",
    "\n",
    "    # create our grid\n",
    "    parameter_grid = [forest_params]\n",
    "    grid_out = GridSearchCV(model_pipeline, parameter_grid, cv=5, verbose=1, n_jobs = -1, scoring=\"accuracy\")\n",
    "    \n",
    "    # fit\n",
    "    grid_out.fit(X_remainder, y_remainder)\n",
    "     \n",
    "    # write results    \n",
    "    file_name = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" GridCVresults.pkl\"\n",
    "    pickle.dump(grid_out, open(file_name, \"wb\") )\n",
    "    print(\"wrote:\",file_name)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
