{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get a feel for the performance of various binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea in this notebook is to get a sense for how various models perform.  The assessment is based simply on the train and test accuracy.\n",
    "\n",
    "In another notebook the models will be evaluated with cross validation and compared with grid searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# my utilities\n",
    "from crash_utils.zip_code_and_borough_from_coords import zip_code_and_borough_from_coords\n",
    "from crash_utils.fix_vehicle_names import fix_vehicle_names\n",
    "from crash_utils.make_crash_features import make_crash_features\n",
    "from crash_utils.basic_cleaning import basic_cleaning\n",
    "from crash_utils.prepare_data_for_modelling import prepare_data_for_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/Mark/brainstation/capstone/nyc_bike_crash_analysis/data/\"\n",
    "df = pd.read_csv(data_path + \"Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing zip coded and boroughs using lat/lon\n",
    "df = zip_code_and_borough_from_coords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the VEHICLE TYPE CODE columns\n",
    "df = fix_vehicle_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some basic data munging operations (see `crash_utils/basic_cleaning.py` for details)\n",
    "df = basic_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modelling\n",
    "# drop columns\n",
    "# set up target\n",
    "# run \"make_crash_features.py\"\n",
    "# OHE the text columns\n",
    "# count-vectorize the vehicles and crash factors\n",
    "\n",
    "df = prepare_data_for_modelling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test some models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the features and targets from the big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix the class imbalance with upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts().sort_index())\n",
    "n_min = np.sum(y_train == 0)\n",
    "n_maj = np.sum(y_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the class imbalance with upsampling\n",
    "\n",
    "minority_mask = y_train == 0\n",
    "\n",
    "X_upsampled, y_upsampled = resample(X_train.loc[minority_mask], \n",
    "                                    y_train.loc[minority_mask], \n",
    "                                    replace = True, \n",
    "                                    n_samples = n_maj)\n",
    "\n",
    "X_train_bal = np.vstack((X_train[y_train == 1], X_upsampled))\n",
    "y_train_bal = np.hstack((y_train[y_train == 1], y_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_train==0))\n",
    "print(np.sum(y_train==1))\n",
    "print(np.sum(y_train_bal==0))\n",
    "print(np.sum(y_train_bal==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run what was found to be the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run what was found to be the best classifier 2020-12-10 00:30:04 GridCVresults.pkl\n",
    "# 2020-12-08, PCA(n_components=20), RandomForestClassifier(max_depth=40, n_estimators = 200, 'scaler': None}\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=40, n_estimators=200)\n",
    "\n",
    "my_pca = PCA(n_components=20).fit(X_train_bal)\n",
    "X_train_bal_pca = my_pca.transform(X_train_bal)\n",
    "X_test_pca = my_pca.transform(X_test)\n",
    "\n",
    "rf.fit(X_train_bal_pca, y_train_bal)\n",
    "\n",
    "rf.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reload the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the class imbalance with upsampling\n",
    "\n",
    "minority_mask = y_train == 0\n",
    "\n",
    "X_upsampled, y_upsampled = resample(X_train.loc[minority_mask], \n",
    "                                    y_train.loc[minority_mask], \n",
    "                                    replace = True, \n",
    "                                    n_samples = n_maj)\n",
    "\n",
    "X_train_bal = np.vstack((X_train[y_train == 1], X_upsampled))\n",
    "y_train_bal = np.hstack((y_train[y_train == 1], y_upsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__scale the data for those algorithms which would benefit (e.g., KNN)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also scale the data for those algorithms which would benefit (e.g., KNN)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_bal)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_bal)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C = 1e-6)\n",
    "lr.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "print(\"training set accuracy\",round(lr.score(X_train_scaled, y_train_bal),3))\n",
    "print(\"test set accuracy\",round(lr.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = 10**np.arange(-9.,5.,1.)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for C in Cs:\n",
    "    lr = LogisticRegression(C = C).fit(X_train_scaled, y_train_bal)\n",
    "    train_score.append(lr.score(X_train_scaled, y_train_bal))\n",
    "    test_score.append(lr.score(X_test_scaled, y_test))\n",
    "    print(C,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at C = \",Cs[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(Cs, train_score, label='training set', marker='o')\n",
    "plt.plot(Cs, test_score, label='test set', marker='o')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel('C (inverse regularization strength)')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Logistic Regression Classification: impact of regularization\")\n",
    "plt.axvline(Cs[max_ind],color=\"k\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break out the coefficient values and feature names for text columns\n",
    "coeffs = lr.coef_.reshape(-1)[253:]\n",
    "features = X.columns[253:]\n",
    "print(features.shape)\n",
    "print(coeffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame({\"coeffs\":coeffs, \"word\":features})\n",
    "word_df.sort_values(by=\"coeffs\",ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.head(20).plot.bar(x = \"word\", rot=90, figsize=(14,8), fontsize=16, legend=None);\n",
    "plt.xlabel(\"Coefficient value\",size=18);\n",
    "plt.ylabel(\"\");\n",
    "plt.title(\"Words that strongly predict injuries\",size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit DTC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=80, min_samples_leaf=100)\n",
    "dtc.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(dtc.score(X_train_bal, y_train_bal),3))\n",
    "print(round(dtc.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = dtc.predict(X_test)\n",
    "y_pred_train= dtc.predict(X_train_bal)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(y_test.value_counts().sort_index())\n",
    "counts_df.rename(columns = {\"outcome\":\"number\"}, inplace=True)\n",
    "counts_df[\"outcome\"] = [\"no injury\",\"injury\"]\n",
    "counts_df.index.name = \"encoding\"\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(conf_matrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__little mini hyperparameter search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depths = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "depths = np.arange(1,211,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for depth in depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth = depth)\n",
    "    dtc.fit(X_train_bal,y_train_bal)\n",
    "    train_score.append(dtc.score(X_train_bal, y_train_bal))\n",
    "    test_score.append(dtc.score(X_test, y_test))\n",
    "    print(depth,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at max_depth of\",depths[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(depths, train_score, label='training set', marker='o')\n",
    "plt.plot(depths, test_score, label='test set', marker='o')\n",
    "#plt.xscale(\"log\")\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"DTC: impact of max depth\")\n",
    "plt.axvline(depths[max_ind],color=\"k\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_leafs = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "min_leafs = np.arange(1,211,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for leaf in min_leafs:\n",
    "    dtc = DecisionTreeClassifier(max_depth = 60, min_samples_leaf = leaf)\n",
    "    dtc.fit(X_train_bal,y_train_bal)\n",
    "    test_score.append(dtc.score(X_test, y_test))\n",
    "    train_score.append(dtc.score(X_train_bal, y_train_bal))\n",
    "    print(leaf,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at min_samples_leaf of\",min_leafs[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(min_leafs, train_score, label='training set', marker='o')\n",
    "plt.plot(min_leafs, test_score, label='test set', marker='o')\n",
    "#plt.xscale(\"log\")\n",
    "plt.xlabel('min samples per leaf')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"DTC: impact of min. samples per leaf\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# optimized RF classifier\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "#rf = RandomForestClassifier(n_jobs = -1)\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(rf.score(X_train_bal, y_train_bal),3))\n",
    "print(round(rf.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.min_samples_leaf)\n",
    "print(rf.max_depth)\n",
    "print(rf.max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depths = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "depths = np.arange(1,181,20)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for depth in depths:\n",
    "    rf = RandomForestClassifier(max_depth = depth, n_estimators = 80, n_jobs = -1)\n",
    "    rf.fit(X_train_bal,y_train_bal)\n",
    "    train_score.append(rf.score(X_train_bal, y_train_bal))\n",
    "    test_score.append(rf.score(X_test, y_test))\n",
    "    print(depth,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at max_depth of\",depths[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(depths, train_score, label='training set', marker='o')\n",
    "plt.plot(depths, test_score, label='test set', marker='o')\n",
    "#plt.xscale(\"log\")\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Random Forest: impact of max depth\")\n",
    "plt.axvline(depths[max_ind],color=\"k\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(10,200,10)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    rf = RandomForestClassifier(max_depth = 80, n_estimators = estimator, n_jobs=-1)\n",
    "    rf.fit(X_train,y_train)\n",
    "    test_score.append(rf.score(X_test, y_test))\n",
    "    train_score.append(rf.score(X_train_bal, y_train_bal))\n",
    "    print(estimator,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at n_estimators of\",estimators[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(estimators, train_score, label='training set', marker='o')\n",
    "plt.plot(estimators, test_score, label='test set', marker='o')\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Random forest: impact of number of estimators\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(gnb.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(gnb.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = gnb.predict(X_test_scaled)\n",
    "y_pred_train= gnb.predict(X_train_scaled)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(gb.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(gb.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(10,200,20)\n",
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "for estimator in estimators:\n",
    "    gb = GradientBoostingClassifier(n_estimators = estimator)\n",
    "    gb.fit(X_train_scaled,y_train_bal)\n",
    "    test_score.append(gb.score(X_test_scaled, y_test))\n",
    "    train_score.append(gb.score(X_train_scaled, y_train_bal))\n",
    "    print(estimator,end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_score)\n",
    "print(\"max test score of\", round(test_score[max_ind],3))\n",
    "print(\"max test score at n_estimators of\",estimators[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result\n",
    "plt.figure()\n",
    "plt.plot(estimators, train_score, label='training set', marker='o')\n",
    "plt.plot(estimators, test_score, label='test set', marker='o')\n",
    "plt.xlabel(\"number of estimators\")\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title(\"Gradient boosting: impact of number of estimators\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#fitting Adaptive Boosting\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(ada.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(ada.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "rc = RidgeClassifier()\n",
    "rc.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(rc.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(rc.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "knn.fit(X_train_scaled, y_train_bal)\n",
    "\n",
    "# score model on train and test sets\n",
    "print(round(knn.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(knn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(hidden_layer_sizes=(32,128,512,128,32), solver='lbfgs')\n",
    "\n",
    "nn.fit(X_train_scaled,y_train_bal)\n",
    "\n",
    "print(round(nn.score(X_train_scaled, y_train_bal),3))\n",
    "print(round(nn.score(X_test_scaled, y_test),3))\n",
    "\n",
    "\n",
    "#nn.fit(X_train_scaled,y_train)\n",
    "\n",
    "# score model on train and test sets\n",
    "# print(round(nn.score(X_train_scaled, y_train),3))\n",
    "# print(round(nn.score(X_test_scaled, y_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and train\n",
    "y_pred_test = nn.predict(X_test_scaled)\n",
    "y_pred_train= nn.predict(X_train_scaled)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "conf_df = pd.DataFrame(data = conf_matrix,\n",
    "                       index = ['True Class 0','True Class 1','True Class 2'],\n",
    "                       columns = ['Predicted Class 0','Predicted Class 1','Predicted Class 2'])\n",
    "\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow/Keras Feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "#model.add(layers.Dense(500, activation=\"relu\"))\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target and features and then train-test-split\n",
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train, y_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "type(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "\n",
    "result = model.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {result[1]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
